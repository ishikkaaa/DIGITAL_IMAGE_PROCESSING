{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# To display plots inline\n",
        "%matplotlib inline\n",
        "\n",
        "# Load the dataset (Assuming the file is named 'heart.csv')\n",
        "df = pd.read_csv('heart.csv')  # No need to upload the file again\n",
        "\n",
        "# Display basic information\n",
        "df.info()\n",
        "\n",
        "# Check for null values\n",
        "print(\"Null values in the dataset:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Correlation matrix\n",
        "corr_matrix = df.corr()\n",
        "\n",
        "# Heatmap of the correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\")\n",
        "plt.title('Correlation Matrix of Heart Disease Dataset')\n",
        "plt.show()\n",
        "\n",
        "# Histograms of each numerical feature\n",
        "df.hist(bins=30, figsize=(15, 10))\n",
        "plt.suptitle('Distribution of Numerical Features')\n",
        "plt.show()\n",
        "\n",
        "# Define features and target (Assume 'target' is the column representing heart disease condition)\n",
        "# Change 'target' to the actual name of the target column in your dataset\n",
        "X = df.drop('target', axis=1)  # Drop target variable\n",
        "y = df['target']  # Target variable (heart disease condition)\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert the Scaled Data to DataFrame for Better Visualization\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X.columns)\n",
        "\n",
        "# Display the First Few Rows of Scaled and Non-Scaled Training and Testing Data\n",
        "\n",
        "# Non-scaled Training and Testing Data\n",
        "print(\"Non-Scaled Training Data:\")\n",
        "print(X_train.head())\n",
        "\n",
        "print(\"\\nNon-Scaled Testing Data:\")\n",
        "print(X_test.head())\n",
        "\n",
        "# Scaled Training and Testing Data\n",
        "print(\"\\nScaled Training Data:\")\n",
        "print(X_train_scaled_df.head())\n",
        "\n",
        "print(\"\\nScaled Testing Data:\")\n",
        "print(X_test_scaled_df.head())\n"
      ],
      "metadata": {
        "id": "M5yIz8hUSicH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}